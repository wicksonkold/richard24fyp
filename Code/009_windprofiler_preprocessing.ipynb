{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "555c065c-4d3c-4844-a1db-345782644f49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "\n",
    "def process_windprofiler(filepath, filename):\n",
    "    ### GET HTML INFO\n",
    "    # Read the HTML content from the file\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the <pre> tag and extract its text\n",
    "    pre_tag = soup.find('pre')\n",
    "    if pre_tag:\n",
    "        text_content = pre_tag.get_text()\n",
    "    else:\n",
    "        text_content = \"No <pre> tag found in the HTML content.\"\n",
    "\n",
    "    # Split the text content into lines\n",
    "    lines = text_content.splitlines()\n",
    "    \n",
    "    ### GET UPPER PART STATIC INFO\n",
    "    # Assuming 'lines' is a list of the lines containing the relevant metadata\n",
    "    metadata = {}\n",
    "    station_line = None\n",
    "    date_line = None\n",
    "\n",
    "    # Find the index of lines which contain 'Station:' and 'Date:' respectively\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'Station:' in line:\n",
    "            station_line = i\n",
    "        if 'Date:' in line:\n",
    "            date_line = i\n",
    "\n",
    "    # Extract information based on identified line\n",
    "    if station_line is not None:\n",
    "        station_match = re.search(r'Station:\\s+(.*?)\\s{2,}', lines[station_line])\n",
    "        if station_match:\n",
    "            if \"Sha Lo Wan - Chek Lap Kok Airpt.\" in station_match.group(1):\n",
    "                station_name = \"SLW\"\n",
    "\n",
    "            if \"Siu Ho Wan\"  in station_match.group(1):\n",
    "                station_name = \"SHW\"\n",
    "\n",
    "            metadata['Station'] = station_name\n",
    "\n",
    "    if date_line is not None:\n",
    "        # Use direct string slicing based on the position\n",
    "        metadata['Date'] = lines[date_line][20:28].strip()\n",
    "\n",
    "    # Convert metadata to string if needed\n",
    "    metadata_str = \"\"\n",
    "    if 'Date' in metadata:\n",
    "        metadata_str += \"Date: {}\\n\".format(metadata['Date'])\n",
    "    if 'Station' in metadata:\n",
    "        metadata_str += \"Station: {}\".format(metadata['Station'])\n",
    "    \n",
    "    \n",
    "    ### Process the lower part for CSV data\n",
    "    # Find the start of the data section\n",
    "    data_start_idx = next(i for i, line in enumerate(lines) if \"(m/s)\" in line) + 1\n",
    "    data_lines = lines[data_start_idx:]\n",
    "    \n",
    "    # CLEAN LINES\n",
    "    def clean_split(line):\n",
    "        # Normalize all sequences of spaces or tabs to exactly three spaces\n",
    "        normalized_line = re.sub(r'[\\t\\s]+', '   ', line.strip())\n",
    "        # Split the line based on three spaces\n",
    "        columns = normalized_line.split('   ')\n",
    "        return columns\n",
    "\n",
    "    # Assuming 'data_lines' is a list of the lines containing the relevant data\n",
    "    data = [clean_split(line) for line in data_lines if line.strip()]\n",
    "\n",
    "    # Convert to a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "    if \"SLW\" in station_name:\n",
    "        headers = [\"Code\", \"height_(m agl)\", \n",
    "                   \"ws_(m/s)\", \n",
    "                   \"wd_(deg)\", \n",
    "                   \"u_(m/s)\", \n",
    "                   \"v_(m/s)\", \n",
    "                   \"w_(m/s)\", \n",
    "                   \"cns_N\", \n",
    "                   \"cns_W\", \n",
    "                   \"cns_V\", \n",
    "                   \"snr_N\", \n",
    "                   \"snr_W\", \n",
    "                   \"snr_V\"]\n",
    "    else:\n",
    "        headers = [\"Code\", \n",
    "                   \"height_(m agl)\", \n",
    "                   \"ws_(m/s)\", \n",
    "                   \"wd_(deg)\", \n",
    "                   \"u_(m/s)\", \n",
    "                   \"v_(m/s)\", \n",
    "                   \"w_(m/s)\", \n",
    "                   \"cns_NE\", \n",
    "                   \"cns_SE\", \n",
    "                   \"cns_V\", \n",
    "                   \"snr_NE\", \n",
    "                   \"snr_SE\", \n",
    "                   \"snr_V\"]\n",
    "    df.columns = headers\n",
    "\n",
    "    ### FILL THE MISSING CODE\n",
    "    def fill_zeros_with_first_value(column):\n",
    "        # Replace ' 0' strings with None\n",
    "        column_replaced = column.apply(lambda x: None if x == ' 0' else x)\n",
    "        # Set non 4-digit cells to None\n",
    "        column_replaced = column_replaced.apply(lambda x: x if x is not None and re.fullmatch(r'\\d{4}', x) else None)\n",
    "        # Forward-fill None values with the first non-None value encountered\n",
    "        column_filled = column_replaced.fillna(method='ffill')\n",
    "        return column_filled\n",
    "\n",
    "    new_df = df.copy()\n",
    "\n",
    "    # Apply the function to the 'Code' column\n",
    "    new_df['Code'] = fill_zeros_with_first_value(new_df['Code'])\n",
    "\n",
    "    # Specify the columns you're interested in\n",
    "    columns_of_interest = [\"u_(m/s)\", \"v_(m/s)\", \"w_(m/s)\"]\n",
    "    # Drop the rows where all elements in the columns of interest are None\n",
    "    new_df = new_df.dropna(subset=columns_of_interest, how='all')\n",
    "    \n",
    "    ### PIVOT TIME\n",
    "    # Prepare the station name by replacing spaces with underscores.\n",
    "    station_name = metadata['Station'].replace(\" \", \"_\")\n",
    "\n",
    "    # Convert date from 'mmddyy' to a 'yyyy-mm-dd' format.\n",
    "    date_str = metadata['Date']\n",
    "    date_dt = pd.to_datetime(date_str, format='%m/%d/%y').strftime('%Y-%m-%d')\n",
    "\n",
    "    # Function to convert the 'Code' to a datetime format.\n",
    "    def convert_to_datetime(code, date_str):\n",
    "        # Add leading zeros to the 'Code' if it's not four digits long.\n",
    "        code_str = str(code).zfill(4)\n",
    "        # Combine the 'Code' with 'date_str' (formatted as 'yyyy-mm-dd').\n",
    "        datetime_str = f\"{date_str} {code_str[:2]}:{code_str[2:]}:00\"\n",
    "        return pd.to_datetime(datetime_str)\n",
    "\n",
    "    # Apply the function to create a new 'datetime' column.\n",
    "    new_df['datetime'] = new_df['Code'].apply(lambda code: convert_to_datetime(code, date_dt))\n",
    "\n",
    "    # Create a multi-index by combining 'datetime' with 'height_(m agl)'.\n",
    "    new_df.set_index(['datetime', 'height_(m agl)'], inplace=True)\n",
    "    \n",
    "    # 重置索引以便 'height_(m agl)' 成为一个列\n",
    "    new_df_reset = new_df.reset_index()\n",
    "    new_df_reset = new_df_reset.drop(columns=['Code'])\n",
    "    # 现在 'height_(m agl)' 应该是列之一，可以进行数据透视\n",
    "    df_pivot = new_df_reset.pivot(index='datetime', columns='height_(m agl)')\n",
    "    # 你可能需要再次重命名列名以便包含站点名称和测量类型\n",
    "    df_pivot.columns = [f\"{station_name}_{col[0]}_{col[1]}\" for col in df_pivot.columns]\n",
    "    df_pivot = df_pivot.reset_index()\n",
    "    \n",
    "    ### OUTPUT \n",
    "    # Save the pivot DataFrame to a CSV file\n",
    "    output_path = \"C:\\\\Users\\\\User\\\\Desktop\\\\Final Year Project\\\\Code\\\\Processed_HKO\\\\processing_windprofiler\"\n",
    "    output_filename = f\"{metadata['Station']}_windprofiler_{filename}_{date_dt.replace('-', '')}.csv\"\n",
    "    df_pivot.to_csv(f\"{output_path}\\\\{output_filename}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86b27f09-00a3-417f-a366-e66c2e5b59ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: C:\\Users\\User\\Desktop\\Final Year Project\\HKO Raw Data\\windprofiler\\2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files in C:\\Users\\User\\Desktop\\Final Year Project\\HKO Raw Data\\windprofiler\\2018: 100%|██████████| 728/728 [03:25<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: C:\\Users\\User\\Desktop\\Final Year Project\\HKO Raw Data\\windprofiler\\2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files in C:\\Users\\User\\Desktop\\Final Year Project\\HKO Raw Data\\windprofiler\\2019: 100%|██████████| 716/716 [03:20<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: C:\\Users\\User\\Desktop\\Final Year Project\\HKO Raw Data\\windprofiler\\2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files in C:\\Users\\User\\Desktop\\Final Year Project\\HKO Raw Data\\windprofiler\\2020:   2%|▏         | 12/524 [00:03<02:33,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process file: clk00114.txt. Error: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files in C:\\Users\\User\\Desktop\\Final Year Project\\HKO Raw Data\\windprofiler\\2020:  19%|█▊        | 98/524 [00:29<02:04,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process file: clk00409.txt. Error: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files in C:\\Users\\User\\Desktop\\Final Year Project\\HKO Raw Data\\windprofiler\\2020: 100%|██████████| 524/524 [02:31<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: C:\\Users\\User\\Desktop\\Final Year Project\\HKO Raw Data\\windprofiler\\2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files in C:\\Users\\User\\Desktop\\Final Year Project\\HKO Raw Data\\windprofiler\\2021:   4%|▍         | 32/731 [00:09<03:29,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process file: clk10202.txt. Error: Length mismatch: Expected axis has 0 elements, new values have 13 elements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files in C:\\Users\\User\\Desktop\\Final Year Project\\HKO Raw Data\\windprofiler\\2021:  95%|█████████▍| 691/731 [03:15<00:10,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process file: shw11121.txt. Error: Index contains duplicate entries, cannot reshape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files in C:\\Users\\User\\Desktop\\Final Year Project\\HKO Raw Data\\windprofiler\\2021: 100%|██████████| 731/731 [03:25<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: C:\\Users\\User\\Desktop\\Final Year Project\\HKO Raw Data\\windprofiler\\2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files in C:\\Users\\User\\Desktop\\Final Year Project\\HKO Raw Data\\windprofiler\\2022:  49%|████▉     | 361/730 [01:48<01:54,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process file: clk21228.txt. Error: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files in C:\\Users\\User\\Desktop\\Final Year Project\\HKO Raw Data\\windprofiler\\2022: 100%|██████████| 730/730 [03:27<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: C:\\Users\\User\\Desktop\\Final Year Project\\HKO Raw Data\\windprofiler\\2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files in C:\\Users\\User\\Desktop\\Final Year Project\\HKO Raw Data\\windprofiler\\2023:  43%|████▎     | 260/608 [01:17<01:42,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process file: clk30918.txt. Error: Length mismatch: Expected axis has 0 elements, new values have 13 elements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files in C:\\Users\\User\\Desktop\\Final Year Project\\HKO Raw Data\\windprofiler\\2023:  98%|█████████▊| 597/608 [02:48<00:02,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process file: shw31020.txt. Error: Index contains duplicate entries, cannot reshape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files in C:\\Users\\User\\Desktop\\Final Year Project\\HKO Raw Data\\windprofiler\\2023: 100%|██████████| 608/608 [02:51<00:00,  3.54it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm  # Make sure to import the tqdm function\n",
    "\n",
    "directories = [\n",
    "    \"C:\\\\Users\\\\User\\\\Desktop\\\\Final Year Project\\\\HKO Raw Data\\\\windprofiler\\\\2018\",\n",
    "    \"C:\\\\Users\\\\User\\\\Desktop\\\\Final Year Project\\\\HKO Raw Data\\\\windprofiler\\\\2019\",\n",
    "    \"C:\\\\Users\\\\User\\\\Desktop\\\\Final Year Project\\\\HKO Raw Data\\\\windprofiler\\\\2020\",\n",
    "    \"C:\\\\Users\\\\User\\\\Desktop\\\\Final Year Project\\\\HKO Raw Data\\\\windprofiler\\\\2021\",\n",
    "    \"C:\\\\Users\\\\User\\\\Desktop\\\\Final Year Project\\\\HKO Raw Data\\\\windprofiler\\\\2022\",\n",
    "    \"C:\\\\Users\\\\User\\\\Desktop\\\\Final Year Project\\\\HKO Raw Data\\\\windprofiler\\\\2023\"\n",
    "]\n",
    "\n",
    "def process_all_windprofiler_files(directory):\n",
    "    # Retrieve a list of text files in the given directory\n",
    "    txt_files = [f for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "\n",
    "    # Loop through the files with tqdm for a progress bar\n",
    "    for filename in tqdm(txt_files, desc=f\"Processing files in {directory}\"):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        new_filename = filename.replace(\".txt\", \"\")\n",
    "\n",
    "        # Process the file\n",
    "        try:\n",
    "            process_windprofiler(file_path, new_filename)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process file: {filename}. Error: {e}\")\n",
    "            \n",
    "# Loop over the directories and process all files in each directory\n",
    "for directory in directories:\n",
    "    print(f\"Processing directory: {directory}\")\n",
    "    process_all_windprofiler_files(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a09a585-f311-4bf6-8989-1365697568fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files starting with SHW_ have been concatenated into C:\\Users\\User\\Desktop\\Final Year Project\\Code\\Processed_HKO\\processed_wp\\SHW_windprofiler_concatenated.csv\n",
      "All files starting with SLW_ have been concatenated into C:\\Users\\User\\Desktop\\Final Year Project\\Code\\Processed_HKO\\processed_wp\\SLW_windprofiler_concatenated.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define input and output directories\n",
    "input_path = \"C:\\\\Users\\\\User\\\\Desktop\\\\Final Year Project\\\\Code\\\\Processed_HKO\\\\processing_windprofiler\"\n",
    "output_path = \"C:\\\\Users\\\\User\\\\Desktop\\\\Final Year Project\\\\Code\\\\Processed_HKO\\\\processed_wp\"\n",
    "\n",
    "# Ensure the output directory exists, create if it doesn't\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# List of prefixes\n",
    "prefixes = [\"SHW_\", \"SLW_\"]\n",
    "\n",
    "for file_prefix in prefixes:\n",
    "    # Initialize an empty list to store DataFrames for the current prefix\n",
    "    df_list = []\n",
    "\n",
    "    # Loop through all the files in the input directory\n",
    "    for filename in os.listdir(input_path):\n",
    "        # Check if the file starts with the current prefix\n",
    "        if filename.startswith(file_prefix):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(input_path, filename)\n",
    "            # Read the CSV file and append it to the list\n",
    "            df_list.append(pd.read_csv(file_path))\n",
    "\n",
    "    # Concatenate all DataFrames in the list into a single DataFrame\n",
    "    concatenated_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # Construct the output file path\n",
    "    concatenated_filename = f\"{file_prefix}windprofiler_concatenated.csv\"\n",
    "    output_file_path = os.path.join(output_path, concatenated_filename)\n",
    "\n",
    "    # Save the concatenated DataFrame to a single CSV file in the output directory\n",
    "    concatenated_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"All files starting with {file_prefix} have been concatenated into {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf96ba0-c0d9-4d1c-a902-e9dd4a07dd93",
   "metadata": {},
   "source": [
    "### CONCAT BOTH SHW SLW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66d20067-4f1b-42f7-a2c0-5ed50c4fd293",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files have been successfully joined and saved to C:\\Users\\User\\Desktop\\Final Year Project\\Code\\Processed_HKO\\processed_wp\\Joined_SHW_SLW_windprofiler.csv\n",
      "CPU times: total: 2min 8s\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the files\n",
    "directory = \"C:\\\\Users\\\\User\\\\Desktop\\\\Final Year Project\\\\Code\\\\Processed_HKO\\\\processed_wp\"\n",
    "\n",
    "# Define filenames based on the prefixes used earlier\n",
    "shw_filename = os.path.join(directory, \"SHW_windprofiler_concatenated.csv\")\n",
    "slw_filename = os.path.join(directory, \"SLW_windprofiler_concatenated.csv\")\n",
    "\n",
    "# Read the files into DataFrames\n",
    "shw_df = pd.read_csv(shw_filename)\n",
    "slw_df = pd.read_csv(slw_filename)\n",
    "\n",
    "# Convert 'datetime' columns to pandas datetime objects and round to the nearest minute\n",
    "shw_df['datetime'] = pd.to_datetime(shw_df['datetime']).dt.round('min')\n",
    "slw_df['datetime'] = pd.to_datetime(slw_df['datetime']).dt.round('min')\n",
    "\n",
    "# Sort the DataFrames by 'datetime' in ascending order\n",
    "shw_df = shw_df.sort_values('datetime', ascending=True)\n",
    "slw_df = slw_df.sort_values('datetime', ascending=True)\n",
    "\n",
    "# Perform the asof merge with 'nearest' direction and within a 3-minute tolerance\n",
    "joined_df = pd.merge_asof(shw_df, slw_df, on='datetime', direction='nearest', tolerance=pd.Timedelta('3min'))\n",
    "\n",
    "# Save the joined DataFrame to a new CSV file\n",
    "output_filename = os.path.join(directory, \"Joined_SHW_SLW_windprofiler.csv\")\n",
    "joined_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"The files have been successfully joined and saved to {output_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
